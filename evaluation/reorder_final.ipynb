{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import IGB260MDGLDataset, OGBDGLDataset\n",
    "print(\"Dataset: OGB\")\n",
    "dataset = OGBDGLDataset(args)\n",
    "g = dataset[0]\n",
    "g  = g.formats('csc')\n",
    "\n",
    "import torch\n",
    "g.ndata['features'] = g.ndata['feat']\n",
    "g.ndata['labels'] = g.ndata['label']\n",
    "\n",
    "train_nid = torch.nonzero(g.ndata['train_mask'], as_tuple=True)[0]\n",
    "print(torch.nonzero(g.ndata['train_mask'], as_tuple=True))\n",
    "val_nid = torch.nonzero(g.ndata['val_mask'], as_tuple=True)[0]\n",
    "test_nid = torch.nonzero(g.ndata['test_mask'], as_tuple=True)[0]\n",
    "in_feats = g.ndata['features'].shape[1]\n",
    "print(train_nid.shape, val_nid.shape, test_nid.shape)\n",
    "\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "from time import perf_counter\n",
    "import torch\n",
    "\n",
    "def ultra_fast_analysis(edge_path):\n",
    "    \"\"\"修复版高效边关系分析（兼容Numba旧版，确保正确性）\"\"\"\n",
    "    print(f\"\\n[阶段1] 数据加载与验证\")\n",
    "    start_load = perf_counter()\n",
    "    raw_data = np.load(edge_path)\n",
    "    if raw_data.shape[0] == 2:\n",
    "        edges = raw_data.T.astype(np.uint32)\n",
    "    else:\n",
    "        edges = raw_data[:, :2].astype(np.uint32)\n",
    "    print(f\"数据加载完成 | 形状：{edges.shape} | 内存占用：{edges.nbytes/1024**3:.1f}GB\")\n",
    "    print(f\"首行样本：{edges[0]} | 末行样本：{edges[-1]}\")\n",
    "    print(f\"加载耗时：{perf_counter() - start_load:.2f}s\")\n",
    "\n",
    "    print(\"\\n[阶段2] 排序（正确顺序）\")\n",
    "    start_sort = perf_counter()\n",
    "    sort_key = (edges[:, 1], edges[:, 0])\n",
    "    sorted_order = np.lexsort(sort_key)\n",
    "    sorted_edges = edges[sorted_order]\n",
    "    print(f\"排序完成 | 首行样本：{sorted_edges[0]}\")\n",
    "    print(f\"排序耗时：{perf_counter() - start_sort:.2f}s\")\n",
    "\n",
    "    print(\"\\n[阶段3] 邻接表构建\")\n",
    "    start_build = perf_counter()\n",
    "    @nb.njit\n",
    "    def compute_offsets(sorted_src, max_src_id):\n",
    "        offsets = np.zeros(max_src_id + 2, dtype=np.uint32)\n",
    "        current = sorted_src[0]\n",
    "        count = 1\n",
    "        for i in range(1, len(sorted_src)):\n",
    "            if sorted_src[i] == current:\n",
    "                count += 1\n",
    "            else:\n",
    "                offsets[current + 1] = count\n",
    "                current = sorted_src[i]\n",
    "                count = 1\n",
    "        offsets[current + 1] = count\n",
    "        for i in range(1, len(offsets)):\n",
    "            offsets[i] += offsets[i - 1]\n",
    "        return offsets\n",
    "    src_nodes = sorted_edges[:, 0]\n",
    "    max_src_id = int(src_nodes.max())\n",
    "    offsets = compute_offsets(src_nodes, max_src_id)\n",
    "    neighbors = sorted_edges[:, 1].astype(np.uint32)\n",
    "    print(f\"构建完成 | 总邻居数：{len(neighbors):,}\")\n",
    "    print(f\"构建耗时：{perf_counter() - start_build:.2f}s\")\n",
    "\n",
    "    print(\"\\n[阶段4] 统计信息生成\")\n",
    "    start_stat = perf_counter()\n",
    "    actual_src_ids = np.unique(src_nodes)\n",
    "    out_degrees = offsets[1:] - offsets[:-1]\n",
    "    valid_degrees = out_degrees[actual_src_ids]\n",
    "    stats = {\n",
    "        \"total_edges\": len(edges),\n",
    "        \"unique_sources\": int(len(actual_src_ids)),\n",
    "        \"max_out_degree\": int(valid_degrees.max()),\n",
    "        \"avg_out_degree\": float(valid_degrees.mean()),\n",
    "        \"median_out_degree\": float(np.median(valid_degrees)),\n",
    "        \"out_degree_distribution\": np.bincount(valid_degrees),\n",
    "        \"original_ids\": actual_src_ids,\n",
    "        \"offsets\": offsets,\n",
    "        \"neighbors\": neighbors\n",
    "    }\n",
    "    print(f\"统计耗时：{perf_counter() - start_stat:.2f}s\")\n",
    "    return stats\n",
    "\n",
    "def add_self_edges_all(original_stats, edges, total_nodes=111059956):\n",
    "    print(\"\\n[自环处理-All] 初始化\")\n",
    "    start = perf_counter()\n",
    "    node_count = total_nodes\n",
    "    full_offsets = original_stats['offsets']\n",
    "    if len(full_offsets) < node_count + 1:\n",
    "        full_offsets = np.concatenate([full_offsets,\n",
    "                                       np.full(node_count + 1 - len(full_offsets), full_offsets[-1], dtype=full_offsets.dtype)])\n",
    "    full_neighbors = original_stats['neighbors']\n",
    "    orig_counts = full_offsets[1:] - full_offsets[:-1]\n",
    "    new_counts = orig_counts + 1\n",
    "    new_offsets = np.empty(node_count + 1, dtype=np.uint32)\n",
    "    new_offsets[0] = 0\n",
    "    new_offsets[1:] = np.cumsum(new_counts)\n",
    "    total_new_edges = int(new_offsets[-1])\n",
    "    new_neighbors = np.empty(total_new_edges, dtype=np.uint32)\n",
    "    print(f\"节点总数：{node_count:,}，分配新邻居数组：{new_neighbors.size:,}\")\n",
    "\n",
    "    @nb.njit(parallel=True)\n",
    "    def fill(full_offsets, full_neighbors, new_offsets, new_neighbors):\n",
    "        n = len(new_offsets) - 1\n",
    "        for i in nb.prange(n):\n",
    "            s, e = full_offsets[i], full_offsets[i+1]\n",
    "            has_self_edge = False\n",
    "            for j in range(e - s):\n",
    "                nbr = full_neighbors[s + j]\n",
    "                if nbr == i:  # 检查是否已包含自环边\n",
    "                    has_self_edge = True\n",
    "                new_neighbors[new_offsets[i] + j] = nbr\n",
    "            if not has_self_edge:\n",
    "                new_neighbors[new_offsets[i+1] - 1] = i  # 如果没有自环，才添加自环边\n",
    "        return new_neighbors\n",
    "    \n",
    "    adjusted_neighbors = fill(full_offsets, full_neighbors, new_offsets, new_neighbors)\n",
    "    print(f\"填充耗时：{perf_counter() - start:.2f}s\")\n",
    "    new_degrees = new_counts\n",
    "    final_stats = original_stats.copy()\n",
    "    final_stats.update({\n",
    "        'offsets': new_offsets,\n",
    "        'neighbors': adjusted_neighbors,\n",
    "        'total_edges': int(original_stats['total_edges'] + node_count),\n",
    "        'max_out_degree': int(new_degrees.max()),\n",
    "        'avg_out_degree': float(new_degrees.mean()),\n",
    "        'median_out_degree': float(np.median(new_degrees)),\n",
    "        'out_degree_distribution': np.bincount(new_degrees),\n",
    "        'unique_sources': int(np.count_nonzero(new_degrees)),\n",
    "        'original_ids': np.arange(node_count, dtype=np.uint32)\n",
    "    })\n",
    "    print(f\"更新统计耗时：{perf_counter() - start:.2f}s\")\n",
    "    return final_stats\n",
    "\n",
    "\n",
    "def split_neighbors_by_remove_indices_enhanced(stats, remove_indices):\n",
    "    offsets = stats['offsets']\n",
    "    neighbors = stats['neighbors']\n",
    "    node_count = len(offsets) - 1\n",
    "    mask = np.zeros(node_count, dtype=bool)\n",
    "    mask[remove_indices] = True\n",
    "    in_lists, out_lists = [], []\n",
    "    for i in range(node_count):\n",
    "        block = neighbors[offsets[i]:offsets[i+1]]\n",
    "        in_lists.append(block[mask[block]])\n",
    "        out_lists.append(block[~mask[block]])\n",
    "    def build(lists):\n",
    "        counts = np.array([len(x) for x in lists], dtype=np.uint32)\n",
    "        offs = np.zeros(node_count+1, dtype=np.uint32)\n",
    "        offs[1:] = np.cumsum(counts)\n",
    "        flat = np.concatenate(lists).astype(np.uint32)\n",
    "        return {\n",
    "            'offsets': offs,\n",
    "            'neighbors': flat,\n",
    "            'original_ids': np.arange(node_count, dtype=np.uint32),\n",
    "            'total_edges': int(flat.size),\n",
    "            'max_out_degree': int(counts.max()) if counts.size else 0,\n",
    "            'avg_out_degree': float(counts.mean()) if counts.size else 0.0,\n",
    "            'median_out_degree': float(np.median(counts)) if counts.size else 0.0,\n",
    "            'unique_sources': int(np.count_nonzero(counts))\n",
    "        }\n",
    "    return build(in_lists), build(out_lists)\n",
    "\n",
    "@nb.njit\n",
    "def _grouping_stage1(\n",
    "    order, offsets, neighbors, used,\n",
    "    neighbor_groups, sources, group_count,\n",
    "    group_size, max_groups_per_node\n",
    "):\n",
    "    \"\"\"\n",
    "    第一阶段：对每个 node 最多生成 max_groups_per_node 组，不复用邻居\n",
    "    \"\"\"\n",
    "    for idx in range(order.shape[0]):\n",
    "        node = order[idx]\n",
    "        # per-node 组计数\n",
    "        node_gp = 0\n",
    "        s, e = offsets[node], offsets[node+1]\n",
    "        tmp = np.empty(group_size, dtype=np.int32)\n",
    "        cnt = 0\n",
    "        for j in range(s, e):\n",
    "            nbr = neighbors[j]\n",
    "            if nbr < used.shape[0] and not used[nbr]:\n",
    "                tmp[cnt] = nbr\n",
    "                cnt += 1\n",
    "                if cnt == group_size:\n",
    "                    # 形成一组\n",
    "                    for k in range(group_size):\n",
    "                        neighbor_groups[group_count[0], k] = tmp[k]\n",
    "                        used[tmp[k]] = 1\n",
    "                    sources[group_count[0]] = node\n",
    "                    group_count[0] += 1\n",
    "                    node_gp += 1\n",
    "                    cnt = 0\n",
    "                    # 达到上限，跳出该 node\n",
    "                    if node_gp >= max_groups_per_node:\n",
    "                        break\n",
    "        # 处理完当前 node，继续下一个\n",
    "    # 返回时 group_count[0] 即为第一阶段实际组数\n",
    "\n",
    "\n",
    "def full_optimized_grouping_multi_var_group_order(\n",
    "    stats, prank_scores,\n",
    "    redundancy_rate=0.2,\n",
    "    group_size=4,\n",
    "    max_groups_per_node=5\n",
    "):\n",
    "    # —— 预处理同原来，得到 offsets, neighbors, order … —— #\n",
    "    # 第一阶段：不复用，每个 node 最多 5 组\n",
    "    orig_off   = stats['offsets'].astype(np.int32)\n",
    "    orig_nei   = stats['neighbors'].astype(np.int32)\n",
    "    total_nodes = orig_off.shape[0] - 1\n",
    "\n",
    "    # —— 预处理：去重并按 prank_scores 排序 —— #\n",
    "    counts = np.empty(total_nodes, dtype=np.int32)\n",
    "    for i in range(total_nodes):\n",
    "        s, e = orig_off[i], orig_off[i+1]\n",
    "        counts[i] = np.unique(orig_nei[s:e]).size\n",
    "\n",
    "    offsets = np.empty_like(orig_off)\n",
    "    offsets[0] = 0\n",
    "    for i in range(1, total_nodes+1):\n",
    "        offsets[i] = offsets[i-1] + counts[i-1]\n",
    "\n",
    "    neighbors = np.empty(offsets[-1], dtype=np.int32)\n",
    "    for i in range(total_nodes):\n",
    "        s, e = orig_off[i], orig_off[i+1]\n",
    "        uniq = np.unique(orig_nei[s:e])\n",
    "        order_idx = np.argsort(-prank_scores[uniq], kind='stable')\n",
    "        neighbors[offsets[i]:offsets[i+1]] = uniq[order_idx]\n",
    "\n",
    "\n",
    "    max_groups = neighbors.size // group_size\n",
    "    ng = np.full((max_groups, group_size), -1, dtype=np.int32)\n",
    "    src = np.full(max_groups, -1, dtype=np.int32)\n",
    "    used = np.zeros(total_nodes, dtype=np.uint8)\n",
    "    gcount = np.zeros(1, dtype=np.int32)\n",
    "\n",
    "    order = np.lexsort(\n",
    "        (\n",
    "            -prank_scores,  # 次排序键：pagerank 降序\n",
    "            -counts         # 主排序键：neighbor count 降序\n",
    "        )\n",
    "    ).astype(np.int32)\n",
    "    _grouping_stage1(\n",
    "        order, offsets, neighbors,\n",
    "        used, ng, src, gcount,\n",
    "        group_size, max_groups_per_node\n",
    "    )\n",
    "\n",
    "    valid_ng  = ng[:gcount[0]]\n",
    "    valid_src = src[:gcount[0]]\n",
    "    missing = total_nodes - int(used.sum())\n",
    "    print(f\"[第一阶段] 组数: {gcount[0]}, 未覆盖: {missing}\")\n",
    "\n",
    "    # —— 第二阶段：允许重用，同样对每个 node 补到最多 5 组 —— #\n",
    "    # 先统计第一阶段每个 node 已有组数\n",
    "    # valid_src 是长度 gcount[0] 的一维数组\n",
    "    counts_per_node = np.zeros(total_nodes, dtype=np.int32)\n",
    "    for s in valid_src:\n",
    "        counts_per_node[s] += 1\n",
    "\n",
    "    new_ng = []\n",
    "    new_src = []\n",
    "    threshold = int(total_nodes * (1 + redundancy_rate))\n",
    "    slots = valid_ng.size\n",
    "\n",
    "    # 只遍历那些组数 < max_groups_per_node 的 node\n",
    "    for node in order:\n",
    "        cur = counts_per_node[node]\n",
    "        if cur >= max_groups_per_node:\n",
    "            continue  # 已经够 5 组\n",
    "        s, e = offsets[node], offsets[node+1]\n",
    "        nbrs = neighbors[s:e]\n",
    "        # 不限 reused，只要组数不足就补\n",
    "        while cur < max_groups_per_node and nbrs.size >= group_size:\n",
    "            sel = nbrs[:group_size]\n",
    "            new_ng.append(sel.copy())\n",
    "            new_src.append(node)\n",
    "            slots += group_size\n",
    "            # 更新 missing / used\n",
    "            for v in sel:\n",
    "                if used[v] == 0:\n",
    "                    missing -= 1\n",
    "                used[v] = 1\n",
    "            nbrs = nbrs[group_size:]\n",
    "            cur += 1\n",
    "        # 如果已达到阈值也可以提前退出\n",
    "        if slots + missing >= threshold:\n",
    "            break\n",
    "\n",
    "    if new_src:\n",
    "        new_ng  = np.vstack(new_ng)\n",
    "        new_src = np.array(new_src, dtype=np.int32)\n",
    "    else:\n",
    "        new_ng  = np.empty((0, group_size), dtype=np.int32)\n",
    "        new_src = np.empty((0,),            dtype=np.int32)\n",
    "\n",
    "    print(f\"[第二阶段] 新增组: {new_src.size}\")\n",
    "\n",
    "    # 合并输出\n",
    "    all_ng  = np.vstack([valid_ng, new_ng])\n",
    "    all_src = np.concatenate([valid_src, new_src])\n",
    "    final_slots = slots + missing\n",
    "    print(f\"最终存储槽位: {final_slots}, 膨胀率: {final_slots/total_nodes:.2f}×\")\n",
    "\n",
    "    return all_src, all_ng\n",
    "\n",
    "\n",
    "# ================== 预计算分组生成 ==================\n",
    "# 1. 加载并分析原始边关系\n",
    "stats = ultra_fast_analysis(\n",
    "    '/mnt/n3/papers100M-bin/processed/edge_index.npy'\n",
    ")\n",
    "\n",
    "# 2. 添加自环\n",
    "stats2 = add_self_edges_all(stats, None)\n",
    "\n",
    "# 3. 移除特定节点索引后的过滤\n",
    "# /home/embed/Documents/gids/evaluation/pr_large_10.pt\n",
    "# /home/embed/Documents/gids/evaluation/wq/pr_large_30.pt\n",
    "remove_idx = torch.load(\n",
    "    '/home/embed/Documents/gids/evaluation/freq_hotness.pt'\n",
    ").numpy()\n",
    "_, filtered = split_neighbors_by_remove_indices_enhanced(stats2, remove_idx)\n",
    "# 4. 加载 PageRank 分数\n",
    "prank = torch.load(\n",
    "    '/home/embed/Documents/gids/evaluation/pr_papers100M.pt'\n",
    ").numpy()\n",
    "\n",
    "pre_srcs, pre_groups = full_optimized_grouping_multi_var_group_order(\n",
    "    stats=filtered,\n",
    "    prank_scores=prank,\n",
    "    redundancy_rate=0.4,        # 总膨胀率 1 + 0.4 = 1.4×\n",
    "    group_size=2,               # 每组 2 个邻居\n",
    "    max_groups_per_node=5       # 每个节点最多 5 组\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b05c6c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标签总数: 111,059,956\n",
      "NaN 标签数量: 109,513,174\n",
      "NaN 占比: 98.61%\n",
      "\n",
      "前几个非 NaN 标签值:\n",
      "[ 79.  26.  80.  35.  80. 118. 102. 132.  27.  88.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 加载 .npy 文件\n",
    "labels = np.load('/mnt/n3/papers100M-bin/processed/node_label.npy')\n",
    "\n",
    "# 将 (N, 1) 的数组变成 (N,) 一维\n",
    "flat_labels = labels.reshape(-1)\n",
    "\n",
    "# 检查 NaN 的占比\n",
    "nan_mask = np.isnan(flat_labels)\n",
    "num_nan = np.sum(nan_mask)\n",
    "num_total = flat_labels.shape[0]\n",
    "\n",
    "print(f\"标签总数: {num_total:,}\")\n",
    "print(f\"NaN 标签数量: {num_nan:,}\")\n",
    "print(f\"NaN 占比: {num_nan / num_total:.2%}\")\n",
    "\n",
    "# 如果需要，可以打印几个非 NaN 的样本看看\n",
    "valid_labels = flat_labels[~nan_mask]\n",
    "print(\"\\n前几个非 NaN 标签值:\")\n",
    "print(valid_labels[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a954f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 成功生成 node_label.npy\n",
      "总节点数: 133,633,040\n",
      "有标签节点数: 1,336,330 (1.00%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 总节点数和有标签数量\n",
    "total_nodes = 133_633_040\n",
    "num_labeled = total_nodes // 100  # 1%\n",
    "\n",
    "# 初始化所有行为 NaN\n",
    "labels = np.full((total_nodes, 1), np.nan, dtype=np.float32)\n",
    "\n",
    "# 随机选择 1% 节点作为有标签节点\n",
    "labeled_indices = np.random.choice(total_nodes, num_labeled, replace=False)\n",
    "\n",
    "# 生成随机标签（假设类别数为 150，可按需调整）\n",
    "num_classes = 19\n",
    "random_labels = np.random.randint(0, num_classes, size=(num_labeled, 1))\n",
    "\n",
    "# 写入标签\n",
    "labels[labeled_indices] = random_labels.astype(np.float32)\n",
    "\n",
    "# 保存为 .npy 文件\n",
    "np.save(\"/home/embed/Documents/Hyperion_notes/dataset/ukunion/node_label.npy\", labels)\n",
    "\n",
    "print(f\"✅ 成功生成 node_label.npy\")\n",
    "print(f\"总节点数: {total_nodes:,}\")\n",
    "print(f\"有标签节点数: {num_labeled:,} ({num_labeled / total_nodes:.2%})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
