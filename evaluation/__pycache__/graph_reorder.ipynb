{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8aebfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== 开始分析 ====\n",
      "\n",
      "[阶段1] 数据加载与验证\n",
      "数据加载完成 | 形状：(12070502, 2) | 内存占用：0.1GB\n",
      "首行样本：[  6731 943344] | 末行样本：[123008 815700]\n",
      "加载耗时：0.08s\n",
      "\n",
      "[阶段2] 排序（正确顺序）\n",
      "排序完成 | 首行样本：[     0 202971]\n",
      "排序耗时：3.43s\n",
      "\n",
      "[阶段3] 邻接表构建\n",
      "构建完成 | 总邻居数：12,070,502\n",
      "构建耗时：0.11s\n",
      "\n",
      "[阶段4] 统计信息生成\n",
      "统计耗时：0.10s\n",
      "\n",
      "[自环处理-All] 初始化\n",
      "节点总数：1,000,000，分配新邻居数组：13,070,502\n",
      "填充耗时：0.20s\n",
      "更新统计耗时：0.22s\n",
      "[孤立节点分组] 共组 2707 组，节点扁平列表长度 10828\n",
      "\n",
      "==== [输入验证器] 初步分析图结构 ====\n",
      "总节点数: 1,000,000\n",
      "总边数: 9,145,723\n",
      "[第一阶段] 有效组数量: 140,825\n",
      "[第一阶段] 组内存储: 563,300, 未覆盖节点: 436,854\n",
      "[第一阶段] 存储膨胀率: 1.00x\n",
      "[第二阶段] 补充组数量: 106,806\n",
      "\n",
      "==== 最终存储统计 ====\n",
      "组内邻居节点数 (计重复): 990,524\n",
      "组外未覆盖节点数: 409,479.0\n",
      "最终总存储节点数: 1,400,003.0\n",
      "总存储膨胀率: 1.40x\n",
      "\n",
      "==== 分组验证结果（允许自环） ====\n",
      "总组数: 247,631\n",
      "验证通过的组数: 247,631\n",
      "验证失败的组数: 0\n",
      "[新节点序列] 总长度 1,400,003 = 10,828 (孤立) + 398,651 (剩余) + 990,524 (原始分组)\n",
      "[自环] 构建后边数: 13,070,502\n",
      "[压缩] 原始边数: 13,070,502, 新边数: 12,385,919\n",
      "[压缩] 原始唯一源节点: 1,000,000, 新唯一源节点: 1,000,000\n",
      "[压缩] 删除分组边: 实际 932,214, 目标 931,968\n",
      "[警告] 删除差异: -246 条\n",
      "[压缩] 添加超级节点边: 实际 247,631, 目标 247,631\n",
      "新节点序列长度: 1,400,003\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numba as nb\n",
    "from time import perf_counter\n",
    "import torch\n",
    "\n",
    "\n",
    "def ultra_fast_analysis(edge_path):\n",
    "    \"\"\"修复版高效边关系分析（兼容Numba旧版，确保正确性）\"\"\"\n",
    "    print(f\"\\n[阶段1] 数据加载与验证\")\n",
    "    start_load = perf_counter()\n",
    "    raw_data = np.load(edge_path)\n",
    "    if raw_data.shape[0] == 2:\n",
    "        edges = raw_data.T.astype(np.uint32)\n",
    "    else:\n",
    "        edges = raw_data[:, :2].astype(np.uint32)\n",
    "    print(f\"数据加载完成 | 形状：{edges.shape} | 内存占用：{edges.nbytes/1024**3:.1f}GB\")\n",
    "    print(f\"首行样本：{edges[0]} | 末行样本：{edges[-1]}\")\n",
    "    print(f\"加载耗时：{perf_counter() - start_load:.2f}s\")\n",
    "\n",
    "    print(\"\\n[阶段2] 排序（正确顺序）\")\n",
    "    start_sort = perf_counter()\n",
    "    sort_key = (edges[:, 1], edges[:, 0])\n",
    "    sorted_order = np.lexsort(sort_key)\n",
    "    sorted_edges = edges[sorted_order]\n",
    "    print(f\"排序完成 | 首行样本：{sorted_edges[0]}\")\n",
    "    print(f\"排序耗时：{perf_counter() - start_sort:.2f}s\")\n",
    "\n",
    "    print(\"\\n[阶段3] 邻接表构建\")\n",
    "    start_build = perf_counter()\n",
    "    @nb.njit\n",
    "    def compute_offsets(sorted_src, max_src_id):\n",
    "        offsets = np.zeros(max_src_id + 2, dtype=np.uint32)\n",
    "        current = sorted_src[0]\n",
    "        count = 1\n",
    "        for i in range(1, len(sorted_src)):\n",
    "            if sorted_src[i] == current:\n",
    "                count += 1\n",
    "            else:\n",
    "                offsets[current + 1] = count\n",
    "                current = sorted_src[i]\n",
    "                count = 1\n",
    "        offsets[current + 1] = count\n",
    "        for i in range(1, len(offsets)):\n",
    "            offsets[i] += offsets[i - 1]\n",
    "        return offsets\n",
    "    src_nodes = sorted_edges[:, 0]\n",
    "    max_src_id = int(src_nodes.max())\n",
    "    offsets = compute_offsets(src_nodes, max_src_id)\n",
    "    neighbors = sorted_edges[:, 1].astype(np.uint32)\n",
    "    print(f\"构建完成 | 总邻居数：{len(neighbors):,}\")\n",
    "    print(f\"构建耗时：{perf_counter() - start_build:.2f}s\")\n",
    "\n",
    "    print(\"\\n[阶段4] 统计信息生成\")\n",
    "    start_stat = perf_counter()\n",
    "    actual_src_ids = np.unique(src_nodes)\n",
    "    out_degrees = offsets[1:] - offsets[:-1]\n",
    "    valid_degrees = out_degrees[actual_src_ids]\n",
    "    stats = {\n",
    "        \"total_edges\": len(edges),\n",
    "        \"unique_sources\": int(len(actual_src_ids)),\n",
    "        \"max_out_degree\": int(valid_degrees.max()),\n",
    "        \"avg_out_degree\": float(valid_degrees.mean()),\n",
    "        \"median_out_degree\": float(np.median(valid_degrees)),\n",
    "        \"out_degree_distribution\": np.bincount(valid_degrees),\n",
    "        \"original_ids\": actual_src_ids,\n",
    "        \"offsets\": offsets,\n",
    "        \"neighbors\": neighbors\n",
    "    }\n",
    "    print(f\"统计耗时：{perf_counter() - start_stat:.2f}s\")\n",
    "    return stats\n",
    "\n",
    "\n",
    "def add_self_edges_all(original_stats, edges, total_nodes=1000000):\n",
    "    print(\"\\n[自环处理-All] 初始化\")\n",
    "    start = perf_counter()\n",
    "    node_count = total_nodes\n",
    "    full_offsets = original_stats['offsets']\n",
    "    if len(full_offsets) < node_count + 1:\n",
    "        full_offsets = np.concatenate([full_offsets,\n",
    "                                       np.full(node_count + 1 - len(full_offsets), full_offsets[-1], dtype=full_offsets.dtype)])\n",
    "    full_neighbors = original_stats['neighbors']\n",
    "    orig_counts = full_offsets[1:] - full_offsets[:-1]\n",
    "    new_counts = orig_counts + 1\n",
    "    new_offsets = np.empty(node_count + 1, dtype=np.uint32)\n",
    "    new_offsets[0] = 0\n",
    "    new_offsets[1:] = np.cumsum(new_counts)\n",
    "    total_new_edges = int(new_offsets[-1])\n",
    "    new_neighbors = np.empty(total_new_edges, dtype=np.uint32)\n",
    "    print(f\"节点总数：{node_count:,}，分配新邻居数组：{new_neighbors.size:,}\")\n",
    "\n",
    "    @nb.njit(parallel=True)\n",
    "    def fill(full_offsets, full_neighbors, new_offsets, new_neighbors):\n",
    "        n = len(new_offsets) - 1\n",
    "        for i in nb.prange(n):\n",
    "            s, e = full_offsets[i], full_offsets[i+1]\n",
    "            has_self_edge = False\n",
    "            for j in range(e - s):\n",
    "                nbr = full_neighbors[s + j]\n",
    "                if nbr == i:  # 检查是否已包含自环边\n",
    "                    has_self_edge = True\n",
    "                new_neighbors[new_offsets[i] + j] = nbr\n",
    "            if not has_self_edge:\n",
    "                new_neighbors[new_offsets[i+1] - 1] = i  # 如果没有自环，才添加自环边\n",
    "        return new_neighbors\n",
    "    \n",
    "    adjusted_neighbors = fill(full_offsets, full_neighbors, new_offsets, new_neighbors)\n",
    "    print(f\"填充耗时：{perf_counter() - start:.2f}s\")\n",
    "    new_degrees = new_counts\n",
    "    final_stats = original_stats.copy()\n",
    "    final_stats.update({\n",
    "        'offsets': new_offsets,\n",
    "        'neighbors': adjusted_neighbors,\n",
    "        'total_edges': int(original_stats['total_edges'] + node_count),\n",
    "        'max_out_degree': int(new_degrees.max()),\n",
    "        'avg_out_degree': float(new_degrees.mean()),\n",
    "        'median_out_degree': float(np.median(new_degrees)),\n",
    "        'out_degree_distribution': np.bincount(new_degrees),\n",
    "        'unique_sources': int(np.count_nonzero(new_degrees)),\n",
    "        'original_ids': np.arange(node_count, dtype=np.uint32)\n",
    "    })\n",
    "    print(f\"更新统计耗时：{perf_counter() - start:.2f}s\")\n",
    "    return final_stats\n",
    "\n",
    "\n",
    "def split_neighbors_by_remove_indices_enhanced(stats, remove_indices):\n",
    "    offsets = stats['offsets']\n",
    "    neighbors = stats['neighbors']\n",
    "    node_count = len(offsets) - 1\n",
    "    mask = np.zeros(node_count, dtype=bool)\n",
    "    mask[remove_indices] = True\n",
    "    in_lists, out_lists = [], []\n",
    "    for i in range(node_count):\n",
    "        block = neighbors[offsets[i]:offsets[i+1]]\n",
    "        in_lists.append(block[mask[block]])\n",
    "        out_lists.append(block[~mask[block]])\n",
    "    def build(lists):\n",
    "        counts = np.array([len(x) for x in lists], dtype=np.uint32)\n",
    "        offs = np.zeros(node_count+1, dtype=np.uint32)\n",
    "        offs[1:] = np.cumsum(counts)\n",
    "        flat = np.concatenate(lists).astype(np.uint32)\n",
    "        return {\n",
    "            'offsets': offs,\n",
    "            'neighbors': flat,\n",
    "            'original_ids': np.arange(node_count, dtype=np.uint32),\n",
    "            'total_edges': int(flat.size),\n",
    "            'max_out_degree': int(counts.max()) if counts.size else 0,\n",
    "            'avg_out_degree': float(counts.mean()) if counts.size else 0.0,\n",
    "            'median_out_degree': float(np.median(counts)) if counts.size else 0.0,\n",
    "            'unique_sources': int(np.count_nonzero(counts))\n",
    "        }\n",
    "    return build(in_lists), build(out_lists)\n",
    "\n",
    "@nb.njit\n",
    "def _grouping_core_with_real_edges(order, offsets, neighbors, used, neighbor_groups, sources, group_count):\n",
    "    for idx in range(len(order)):\n",
    "        node = order[idx]\n",
    "        if used[node]: continue\n",
    "        s, e = offsets[node], offsets[node+1]\n",
    "        cnt = 0\n",
    "        for j in range(s, e):\n",
    "            nbr = neighbors[j]\n",
    "            if nbr < len(used) and not used[nbr]:\n",
    "                neighbor_groups[group_count[0], cnt] = nbr\n",
    "                cnt += 1\n",
    "                if cnt == 4: break\n",
    "        if cnt == 4:\n",
    "            sources[group_count[0]] = node\n",
    "            group_count[0] += 1\n",
    "            for k in range(4):\n",
    "                used[neighbor_groups[group_count[0]-1, k]] = 1\n",
    "\n",
    "\n",
    "def full_optimized_grouping_with_real_edges(stats, prank_scores, redundancy_rate=0.2):\n",
    "    offsets = stats['offsets'].astype(np.int32)\n",
    "    neighbors = stats['neighbors'].astype(np.int32)\n",
    "    total_nodes = len(offsets) - 1\n",
    "    total_edges = neighbors.size\n",
    "    degrees = offsets[1:] - offsets[:-1]\n",
    "    print(\"\\n==== [输入验证器] 初步分析图结构 ====\")\n",
    "    print(f\"总节点数: {total_nodes:,}\")\n",
    "    print(f\"总边数: {total_edges:,}\")\n",
    "\n",
    "    # 第1阶段：正常分组\n",
    "    initial_nodes = total_nodes\n",
    "    max_groups = total_edges // 4\n",
    "    neighbor_groups = np.full((max_groups, 4), -1, dtype=np.int32)\n",
    "    sources = np.full(max_groups, -1, dtype=np.int32)\n",
    "    used = np.zeros(total_nodes, dtype=np.uint8)\n",
    "    group_count = np.zeros(1, dtype=np.int32)\n",
    "    order = np.lexsort((-prank_scores, degrees)).astype(np.int32)\n",
    "    _grouping_core_with_real_edges(order, offsets, neighbors, used, neighbor_groups, sources, group_count)\n",
    "    valid_ng = neighbor_groups[:group_count[0]]\n",
    "    valid_src = sources[:group_count[0]]\n",
    "    group_storage = valid_ng.size\n",
    "    used_count = int(used.sum())\n",
    "    missing_count = total_nodes - used_count\n",
    "    first_storage = group_storage + missing_count\n",
    "    print(f\"[第一阶段] 有效组数量: {group_count[0]:,}\")\n",
    "    print(f\"[第一阶段] 组内存储: {group_storage:,}, 未覆盖节点: {missing_count:,}\")\n",
    "    print(f\"[第一阶段] 存储膨胀率: {first_storage/total_nodes:.2f}x\")\n",
    "\n",
    "    # 第2阶段：真实边冗余补组\n",
    "    new_ng, new_src = [], []\n",
    "    for node in np.where(used == 0)[0]:\n",
    "        current_storage = group_storage + (total_nodes - used_count)\n",
    "        if current_storage / total_nodes >= 1 + redundancy_rate:\n",
    "            break\n",
    "        nbrs = neighbors[offsets[node]:offsets[node+1]]\n",
    "        if len(nbrs) >= 4:\n",
    "            selected = nbrs[:4]\n",
    "            new_ng.append(selected)\n",
    "            new_src.append(node)\n",
    "            group_storage += 4\n",
    "            for nbr in selected:\n",
    "                if not used[nbr]:\n",
    "                    used[nbr] = 1\n",
    "                    used_count += 1\n",
    "    new_count = len(new_src)\n",
    "    if new_count:\n",
    "        new_ng = np.vstack(new_ng)\n",
    "        new_src = np.array(new_src, dtype=np.int32)\n",
    "    else:\n",
    "        new_ng = np.empty((0,4), dtype=np.int32)\n",
    "        new_src = np.empty((0,), dtype=np.int32)\n",
    "    print(f\"[第二阶段] 补充组数量: {new_count:,}\")\n",
    "\n",
    "    # 合并并统计\n",
    "    all_ng = np.vstack([valid_ng, new_ng])\n",
    "    all_src = np.concatenate([valid_src, new_src])\n",
    "    group_nodes = all_ng.flatten()\n",
    "    final_missing = total_nodes - used.sum()\n",
    "    final_storage = group_nodes.size + final_missing\n",
    "    print(\"\\n==== 最终存储统计 ====\")\n",
    "    print(f\"组内邻居节点数 (计重复): {group_nodes.size:,}\")\n",
    "    print(f\"组外未覆盖节点数: {final_missing:,}\")\n",
    "    print(f\"最终总存储节点数: {final_storage:,}\")\n",
    "    print(f\"总存储膨胀率: {final_storage/total_nodes:.2f}x\")\n",
    "\n",
    "    return all_src, all_ng\n",
    "\n",
    "def validate_groups_with_edges_allow_self_loop(all_sources, all_neighbor_groups, edges):\n",
    "    edge_set = set((int(s), int(d)) for s, d in edges)\n",
    "    total = len(all_sources)\n",
    "    valid = 0\n",
    "    for i in range(total):\n",
    "        src = int(all_sources[i])\n",
    "        nbrs = all_neighbor_groups[i]\n",
    "        ok = True\n",
    "        for d in nbrs:\n",
    "            if src == d:\n",
    "                continue\n",
    "            if (src, int(d)) not in edge_set:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            valid += 1\n",
    "    print(\"\\n==== 分组验证结果（允许自环） ====\")\n",
    "    print(f\"总组数: {total:,}\")\n",
    "    print(f\"验证通过的组数: {valid:,}\")\n",
    "    print(f\"验证失败的组数: {total-valid:,}\")\n",
    "    return valid, total-valid\n",
    "\n",
    "\n",
    "def build_edges_from_stats(stats):\n",
    "    offsets, neighbors = stats['offsets'], stats['neighbors']\n",
    "    total_edges = neighbors.size\n",
    "    arr = np.empty((total_edges, 2), dtype=np.int32)\n",
    "    idx = 0\n",
    "    for src in range(len(offsets)-1):\n",
    "        for j in range(offsets[src], offsets[src+1]):\n",
    "            arr[idx] = [src, neighbors[j]]\n",
    "            idx += 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "def compress_edges_with_supernodes(edge_path, srcs, groups, total_nodes, out_path='edge_index_super.npy'):\n",
    "    \"\"\"\n",
    "    用超级节点替换原始分组边（向量化实现）\n",
    "    - 删除所有分组边\n",
    "    - 添加每组 src->supernode 边\n",
    "    \"\"\"\n",
    "    import numpy as _np\n",
    "    raw = _np.load(edge_path)\n",
    "    orig = raw.T if raw.shape[0]==2 else raw[:, :2]\n",
    "    orig0 = orig[:,0].astype(_np.int64); orig1 = orig[:,1].astype(_np.int64)\n",
    "    base = total_nodes + len(srcs)\n",
    "    orig_keys = orig0 * base + orig1\n",
    "    sd0 = _np.array(srcs, dtype=_np.int64)\n",
    "    sd1 = _np.array(groups).flatten().astype(_np.int64)\n",
    "    delete_keys = sd0.repeat(4) * base + sd1\n",
    "    delete_unique = _np.unique(delete_keys)\n",
    "    mask = _np.isin(orig_keys, delete_unique)\n",
    "    kept = orig[~mask]\n",
    "    super_ids = _np.arange(len(srcs), dtype=_np.int64) + total_nodes\n",
    "    src_ids = _np.array(srcs, dtype=_np.int64)\n",
    "    new_rows = _np.stack([src_ids, super_ids], axis=1)\n",
    "    new_arr = _np.vstack([kept, new_rows]).astype(_np.int32)\n",
    "    _np.save(out_path, new_arr)\n",
    "    print(f\"[压缩] 原始边数: {orig.shape[0]:,}, 新边数: {new_arr.shape[0]:,}\")\n",
    "    print(f\"[压缩] 原始唯一源节点: {len(_np.unique(orig[:,0])):,}, 新唯一源节点: {len(_np.unique(new_arr[:,0])):,}\")\n",
    "    actual_deleted = mask.sum()\n",
    "    expected_deleted = delete_unique.size\n",
    "    print(f\"[压缩] 删除分组边: 实际 {actual_deleted:,}, 目标 {expected_deleted:,}\")\n",
    "    if actual_deleted != expected_deleted:\n",
    "        print(f\"[警告] 删除差异: {expected_deleted - actual_deleted} 条\")\n",
    "    actual_added = new_rows.shape[0]\n",
    "    print(f\"[压缩] 添加超级节点边: 实际 {actual_added:,}, 目标 {len(srcs):,}\")\n",
    "    return new_arr\n",
    "\n",
    "def group_isolated_nodes(stats, train_limit=600_000):\n",
    "    \"\"\"\n",
    "    将训练集 [0, train_limit) 中仅含自环的孤立节点，\n",
    "    按 4 个一组扁平展开返回。\n",
    "    丢弃无法凑满 4 个的尾部节点。\n",
    "    \"\"\"\n",
    "    offsets = stats['offsets']\n",
    "    # 计算每个节点的出度（包含自环）\n",
    "    out_degrees = offsets[1:] - offsets[:-1]\n",
    "    # 筛选训练集范围内度==1 的孤立节点\n",
    "    train_nodes = np.arange(train_limit, dtype=np.uint32)\n",
    "    iso_mask = (out_degrees[train_nodes] == 1)\n",
    "    isolated = train_nodes[iso_mask]\n",
    "    # 丢弃尾部不能凑满 4 的节点\n",
    "    num_full = (len(isolated) // 4) * 4\n",
    "    isolated = isolated[:num_full]\n",
    "    # 按组 reshape 再扁平\n",
    "    if isolated.size == 0:\n",
    "        return np.empty((0,), dtype=np.uint32)\n",
    "    iso_grouped_flat = isolated.reshape(-1, 4).flatten()\n",
    "    return iso_grouped_flat\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"==== 开始分析 ====\")\n",
    "    stats = ultra_fast_analysis('/home/embed/Downloads/igb_datasets/small/processed/paper__cites__paper/edge_index.npy')\n",
    "    stats2 = add_self_edges_all(stats, None)\n",
    "\n",
    "    iso_grouped = group_isolated_nodes(stats2, train_limit=600000)\n",
    "    print(f\"[孤立节点分组] 共组 {len(iso_grouped)//4} 组，节点扁平列表长度 {iso_grouped.size}\")\n",
    "\n",
    "    remove_idx = torch.load('pr_small_30.pt').numpy()\n",
    "    _, filtered = split_neighbors_by_remove_indices_enhanced(stats2, remove_idx)\n",
    "    prank = torch.load('pr_small.pt').numpy()\n",
    "    srcs, groups = full_optimized_grouping_with_real_edges(filtered, prank, redundancy_rate=0.4)\n",
    "    \n",
    "    raw = np.load('/home/embed/Downloads/igb_datasets/small/processed/paper__cites__paper/edge_index.npy')\n",
    "    edges = raw.T if raw.shape[0]==2 else raw[:, :2]\n",
    "    validate_groups_with_edges_allow_self_loop(srcs, groups, edges)\n",
    "\n",
    "    # —— 合并三类节点顺序 —— \n",
    "    grouped_nodes = groups.flatten()\n",
    "    total_nodes = len(stats2['offsets']) - 1\n",
    "    used_iso  = np.zeros(total_nodes, dtype=bool)\n",
    "    used_iso[iso_grouped] = True\n",
    "    used_orig = np.zeros(total_nodes, dtype=bool)\n",
    "    used_orig[grouped_nodes] = True\n",
    "    remaining = np.where(~(used_iso | used_orig))[0].astype(np.uint32)\n",
    "\n",
    "    new_node_sequence = np.concatenate([\n",
    "        iso_grouped,\n",
    "        remaining,\n",
    "        grouped_nodes\n",
    "    ])\n",
    "    print(f\"[新节点序列] 总长度 {new_node_sequence.size:,} = \"\n",
    "          f\"{iso_grouped.size:,} (孤立) + {remaining.size:,} (剩余) + {grouped_nodes.size:,} (原始分组)\")\n",
    "    \n",
    "    # 生成新的节点序列和分组关系\n",
    "    edges_sl = build_edges_from_stats(stats2)\n",
    "    print(f\"[自环] 构建后边数: {edges_sl.shape[0]:,}\")\n",
    "    np.save('edge_index_with_self.npy', edges_sl)\n",
    "    compress_edges_with_supernodes('edge_index_with_self.npy', srcs, groups, total_nodes)\n",
    "\n",
    "    #grouped_nodes = groups.flatten()\n",
    "    #used_mask = np.zeros(total_nodes, dtype=bool)\n",
    "    #used_mask[grouped_nodes] = True\n",
    "    #remaining_nodes = np.where(~used_mask)[0]\n",
    "    #new_node_sequence = np.concatenate([grouped_nodes, remaining_nodes])\n",
    "    #grouping_relation = [(int(srcs[i]), [int(n) for n in groups[i]]) for i in range(len(srcs))]\n",
    "    # 保存为文件\n",
    "    #np.save('new_node_sequence.npy', new_node_sequence)\n",
    "    #np.save('grouping_relation_srcs.npy', srcs)\n",
    "    #np.save('grouping_relation_ngs.npy', groups)\n",
    "    # 输出 new_node_sequence 的节点数目\n",
    "    print(f\"新节点序列长度: {len(new_node_sequence):,}\")\n",
    "    #print(\"已保存：new_node_sequence.npy, grouping_relation_srcs.npy, grouping_relation_ngs.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bdd2a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_node_features_from_array(feats, new_seq, out_feat_path, batch_size=100_000):\n",
    "    \"\"\"\n",
    "    根据 new_node_sequence 数组重新排序节点特征，并保存为新的 .npy 文件。\n",
    "\n",
    "    feats: 原始特征数组或 memmap，shape=(M, F)\n",
    "    new_seq: 新节点序列索引数组，shape=(K,)，各元素需在 [0, M)\n",
    "    out_feat_path: 输出特征文件路径（.npy）\n",
    "    batch_size: 每批次处理的节点数量，默认 100k\n",
    "    \"\"\"\n",
    "    M, F = feats.shape\n",
    "    K = len(new_seq)\n",
    "    print(f\"[自定义阶段] 重排序节点特征: 原始节点={M:,}, 输出节点={K:,}, 特征维度={F}\")\n",
    "\n",
    "    # 为输出创建 memmap 文件，shape=(K, F)\n",
    "    reordered = np.lib.format.open_memmap(\n",
    "        out_feat_path,\n",
    "        mode='w+',\n",
    "        dtype=feats.dtype,\n",
    "        shape=(K, F)\n",
    "    )\n",
    "\n",
    "    # 分批拷贝\n",
    "    for start in range(0, K, batch_size):\n",
    "        end = min(start + batch_size, K)\n",
    "        reordered[start:end] = feats[new_seq[start:end]]\n",
    "        #print(f\"已处理节点特征: {start} 到 {end} / {K}\")\n",
    "\n",
    "    print(f\"重排序完成，已保存至 {out_feat_path} | 形状: {reordered.shape}\")\n",
    "    return reordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2df676ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[自定义阶段] 重排序节点特征: 原始节点=1,000,000, 输出节点=1,200,002, 特征维度=1024\n",
      "重排序完成，已保存至 node_feat_reordered.npy | 形状: (1200002, 1024)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "memmap([[ 0.02515233, -0.01264382,  0.04102647, ..., -0.0205105 ,\n",
       "          0.08211689,  0.04823443],\n",
       "        [ 0.00428829,  0.00132501,  0.02884917, ..., -0.00682651,\n",
       "          0.02731467, -0.00288911],\n",
       "        [-0.00642104,  0.00703076,  0.01550619, ...,  0.02637981,\n",
       "          0.02820925,  0.08262179],\n",
       "        ...,\n",
       "        [ 0.02562011, -0.01205072,  0.0342802 , ...,  0.00325665,\n",
       "          0.03307772, -0.03895159],\n",
       "        [ 0.01278548, -0.02280782,  0.00861824, ..., -0.01126363,\n",
       "         -0.01571409, -0.02615075],\n",
       "        [-0.02697082, -0.01159495, -0.00284608, ..., -0.02513741,\n",
       "          0.01619816,  0.02142584]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = np.load('/home/embed/Downloads/igb_datasets/small/processed/paper/node_feat.npy', mmap_mode='r')\n",
    "reorder_node_features_from_array(\n",
    "    feats=feats,\n",
    "    new_seq=new_node_sequence,\n",
    "    out_feat_path='node_feat_reordered.npy'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7601d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_original_isolated_nodes(stats, train_limit=600_000):\n",
    "    \"\"\"\n",
    "    统计 stats（原始图，不含自环）中，\n",
    "    训练集 [0, train_limit) 里，出度==0 的孤立节点数量。\n",
    "    \"\"\"\n",
    "    offsets = stats['offsets']\n",
    "    degrees = offsets[1:] - offsets[:-1]\n",
    "    # 只看训练节点\n",
    "    train_nodes = np.arange(train_limit, dtype=np.uint32)\n",
    "    iso_count = int((degrees[train_nodes] == 0).sum())\n",
    "    return iso_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a7c5d",
   "metadata": {},
   "source": [
    "###        for igb-large test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f96024ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba as nb\n",
    "from time import perf_counter\n",
    "import torch\n",
    "\n",
    "\n",
    "def ultra_fast_analysis(edge_path):\n",
    "    \"\"\"修复版高效边关系分析（兼容Numba旧版，确保正确性）\"\"\"\n",
    "    print(f\"\\n[阶段1] 数据加载与验证\")\n",
    "    start_load = perf_counter()\n",
    "    raw_data = np.load(edge_path)\n",
    "    if raw_data.shape[0] == 2:\n",
    "        edges = raw_data.T.astype(np.uint32)\n",
    "    else:\n",
    "        edges = raw_data[:, :2].astype(np.uint32)\n",
    "    print(f\"数据加载完成 | 形状：{edges.shape} | 内存占用：{edges.nbytes/1024**3:.1f}GB\")\n",
    "    print(f\"首行样本：{edges[0]} | 末行样本：{edges[-1]}\")\n",
    "    print(f\"加载耗时：{perf_counter() - start_load:.2f}s\")\n",
    "\n",
    "    print(\"\\n[阶段2] 排序（正确顺序）\")\n",
    "    start_sort = perf_counter()\n",
    "    sort_key = (edges[:, 1], edges[:, 0])\n",
    "    sorted_order = np.lexsort(sort_key)\n",
    "    sorted_edges = edges[sorted_order]\n",
    "    print(f\"排序完成 | 首行样本：{sorted_edges[0]}\")\n",
    "    print(f\"排序耗时：{perf_counter() - start_sort:.2f}s\")\n",
    "\n",
    "    print(\"\\n[阶段3] 邻接表构建\")\n",
    "    start_build = perf_counter()\n",
    "    @nb.njit\n",
    "    def compute_offsets(sorted_src, max_src_id):\n",
    "        offsets = np.zeros(max_src_id + 2, dtype=np.uint32)\n",
    "        current = sorted_src[0]\n",
    "        count = 1\n",
    "        for i in range(1, len(sorted_src)):\n",
    "            if sorted_src[i] == current:\n",
    "                count += 1\n",
    "            else:\n",
    "                offsets[current + 1] = count\n",
    "                current = sorted_src[i]\n",
    "                count = 1\n",
    "        offsets[current + 1] = count\n",
    "        for i in range(1, len(offsets)):\n",
    "            offsets[i] += offsets[i - 1]\n",
    "        return offsets\n",
    "    src_nodes = sorted_edges[:, 0]\n",
    "    max_src_id = int(src_nodes.max())\n",
    "    offsets = compute_offsets(src_nodes, max_src_id)\n",
    "    neighbors = sorted_edges[:, 1].astype(np.uint32)\n",
    "    print(f\"构建完成 | 总邻居数：{len(neighbors):,}\")\n",
    "    print(f\"构建耗时：{perf_counter() - start_build:.2f}s\")\n",
    "\n",
    "    print(\"\\n[阶段4] 统计信息生成\")\n",
    "    start_stat = perf_counter()\n",
    "    actual_src_ids = np.unique(src_nodes)\n",
    "    out_degrees = offsets[1:] - offsets[:-1]\n",
    "    valid_degrees = out_degrees[actual_src_ids]\n",
    "    stats = {\n",
    "        \"total_edges\": len(edges),\n",
    "        \"unique_sources\": int(len(actual_src_ids)),\n",
    "        \"max_out_degree\": int(valid_degrees.max()),\n",
    "        \"avg_out_degree\": float(valid_degrees.mean()),\n",
    "        \"median_out_degree\": float(np.median(valid_degrees)),\n",
    "        \"out_degree_distribution\": np.bincount(valid_degrees),\n",
    "        \"original_ids\": actual_src_ids,\n",
    "        \"offsets\": offsets,\n",
    "        \"neighbors\": neighbors\n",
    "    }\n",
    "    print(f\"统计耗时：{perf_counter() - start_stat:.2f}s\")\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b636a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[阶段1] 数据加载与验证\n",
      "数据加载完成 | 形状：(1223571364, 2) | 内存占用：9.1GB\n",
      "首行样本：[92733097 55034722] | 末行样本：[54290551 13962549]\n",
      "加载耗时：8.32s\n",
      "\n",
      "[阶段2] 排序（正确顺序）\n",
      "排序完成 | 首行样本：[     0 847930]\n",
      "排序耗时：565.08s\n",
      "\n",
      "[阶段3] 邻接表构建\n",
      "构建完成 | 总邻居数：1,223,571,364\n",
      "构建耗时：4.38s\n",
      "\n",
      "[阶段4] 统计信息生成\n",
      "统计耗时：18.69s\n"
     ]
    }
   ],
   "source": [
    "stats = ultra_fast_analysis('/home/embed/Downloads/igb_large/processed/paper__cites__paper/edge_index.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3f1eb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_self_edges_all(original_stats, edges, total_nodes=100_000_000):\n",
    "    print(\"\\n[自环处理-All] 初始化\")\n",
    "    start = perf_counter()\n",
    "    node_count = total_nodes\n",
    "    full_offsets = original_stats['offsets']\n",
    "    if len(full_offsets) < node_count + 1:\n",
    "        full_offsets = np.concatenate([full_offsets,\n",
    "                                       np.full(node_count + 1 - len(full_offsets), full_offsets[-1], dtype=full_offsets.dtype)])\n",
    "    full_neighbors = original_stats['neighbors']\n",
    "    orig_counts = full_offsets[1:] - full_offsets[:-1]\n",
    "    new_counts = orig_counts + 1\n",
    "    new_offsets = np.empty(node_count + 1, dtype=np.uint32)\n",
    "    new_offsets[0] = 0\n",
    "    new_offsets[1:] = np.cumsum(new_counts)\n",
    "    total_new_edges = int(new_offsets[-1])\n",
    "    new_neighbors = np.empty(total_new_edges, dtype=np.uint32)\n",
    "    print(f\"节点总数：{node_count:,}，分配新邻居数组：{new_neighbors.size:,}\")\n",
    "    @nb.njit(parallel=True)\n",
    "    def fill(full_offsets, full_neighbors, new_offsets, new_neighbors):\n",
    "        n = len(new_offsets) - 1\n",
    "        for i in nb.prange(n):\n",
    "            s, e = full_offsets[i], full_offsets[i+1]\n",
    "            for j in range(e - s):\n",
    "                new_neighbors[new_offsets[i] + j] = full_neighbors[s + j]\n",
    "            new_neighbors[new_offsets[i+1] - 1] = i\n",
    "        return new_neighbors\n",
    "    adjusted_neighbors = fill(full_offsets, full_neighbors, new_offsets, new_neighbors)\n",
    "    print(f\"填充耗时：{perf_counter() - start:.2f}s\")\n",
    "    new_degrees = new_counts\n",
    "    final_stats = original_stats.copy()\n",
    "    final_stats.update({\n",
    "        'offsets': new_offsets,\n",
    "        'neighbors': adjusted_neighbors,\n",
    "        'total_edges': int(original_stats['total_edges'] + node_count),\n",
    "        'max_out_degree': int(new_degrees.max()),\n",
    "        'avg_out_degree': float(new_degrees.mean()),\n",
    "        'median_out_degree': float(np.median(new_degrees)),\n",
    "        'out_degree_distribution': np.bincount(new_degrees),\n",
    "        'unique_sources': int(np.count_nonzero(new_degrees)),\n",
    "        'original_ids': np.arange(node_count, dtype=np.uint32)\n",
    "    })\n",
    "    print(f\"更新统计耗时：{perf_counter() - start:.2f}s\")\n",
    "    return final_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e089f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[自环处理-All] 初始化\n",
      "节点总数：100,000,000，分配新邻居数组：1,323,571,364\n",
      "填充耗时：1.56s\n",
      "更新统计耗时：2.56s\n"
     ]
    }
   ],
   "source": [
    "stats2 = add_self_edges_all(stats, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f25d396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_isolated_nodes(stats, train_limit=600_000):\n",
    "    \"\"\"\n",
    "    将训练集 [0, train_limit) 中仅含自环的孤立节点，\n",
    "    按 4 个一组扁平展开返回。\n",
    "    丢弃无法凑满 4 个的尾部节点。\n",
    "    \"\"\"\n",
    "    offsets = stats['offsets']\n",
    "    # 计算每个节点的出度（包含自环）\n",
    "    out_degrees = offsets[1:] - offsets[:-1]\n",
    "    # 筛选训练集范围内度==1 的孤立节点\n",
    "    train_nodes = np.arange(train_limit, dtype=np.uint32)\n",
    "    iso_mask = (out_degrees[train_nodes] == 1)\n",
    "    isolated = train_nodes[iso_mask]\n",
    "    # 丢弃尾部不能凑满 4 的节点\n",
    "    num_full = (len(isolated) // 4) * 4\n",
    "    isolated = isolated[:num_full]\n",
    "    # 按组 reshape 再扁平\n",
    "    if isolated.size == 0:\n",
    "        return np.empty((0,), dtype=np.uint32)\n",
    "    iso_grouped_flat = isolated.reshape(-1, 4).flatten()\n",
    "    return iso_grouped_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f57192b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[孤立节点分组] 共组 7257862 组，节点扁平列表长度 29031448\n"
     ]
    }
   ],
   "source": [
    "iso_grouped = group_isolated_nodes(stats2, train_limit=60_000_000)\n",
    "print(f\"[孤立节点分组] 共组 {len(iso_grouped)//4} 组，节点扁平列表长度 {iso_grouped.size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb0087f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_neighbors_by_remove_indices_enhanced(stats, remove_indices):\n",
    "    offsets = stats['offsets']\n",
    "    neighbors = stats['neighbors']\n",
    "    node_count = len(offsets) - 1\n",
    "    mask = np.zeros(node_count, dtype=bool)\n",
    "    mask[remove_indices] = True\n",
    "    in_lists, out_lists = [], []\n",
    "    for i in range(node_count):\n",
    "        block = neighbors[offsets[i]:offsets[i+1]]\n",
    "        in_lists.append(block[mask[block]])\n",
    "        out_lists.append(block[~mask[block]])\n",
    "    def build(lists):\n",
    "        counts = np.array([len(x) for x in lists], dtype=np.uint32)\n",
    "        offs = np.zeros(node_count+1, dtype=np.uint32)\n",
    "        offs[1:] = np.cumsum(counts)\n",
    "        flat = np.concatenate(lists).astype(np.uint32)\n",
    "        return {\n",
    "            'offsets': offs,\n",
    "            'neighbors': flat,\n",
    "            'original_ids': np.arange(node_count, dtype=np.uint32),\n",
    "            'total_edges': int(flat.size),\n",
    "            'max_out_degree': int(counts.max()) if counts.size else 0,\n",
    "            'avg_out_degree': float(counts.mean()) if counts.size else 0.0,\n",
    "            'median_out_degree': float(np.median(counts)) if counts.size else 0.0,\n",
    "            'unique_sources': int(np.count_nonzero(counts))\n",
    "        }\n",
    "    return build(in_lists), build(out_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d5ea63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_idx = torch.load('pr_large_30.pt').numpy()\n",
    "_, filtered = split_neighbors_by_remove_indices_enhanced(stats2, remove_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaef8ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.njit\n",
    "def _grouping_core_with_real_edges(order, offsets, neighbors, used, neighbor_groups, sources, group_count):\n",
    "    for idx in range(len(order)):\n",
    "        node = order[idx]\n",
    "        if used[node]: continue\n",
    "        s, e = offsets[node], offsets[node+1]\n",
    "        cnt = 0\n",
    "        for j in range(s, e):\n",
    "            nbr = neighbors[j]\n",
    "            if nbr < len(used) and not used[nbr]:\n",
    "                neighbor_groups[group_count[0], cnt] = nbr\n",
    "                cnt += 1\n",
    "                if cnt == 4: break\n",
    "        if cnt == 4:\n",
    "            sources[group_count[0]] = node\n",
    "            group_count[0] += 1\n",
    "            for k in range(4):\n",
    "                used[neighbor_groups[group_count[0]-1, k]] = 1\n",
    "\n",
    "\n",
    "def full_optimized_grouping_with_real_edges(stats, prank_scores, redundancy_rate=0.2):\n",
    "    offsets = stats['offsets'].astype(np.int32)\n",
    "    neighbors = stats['neighbors'].astype(np.int32)\n",
    "    total_nodes = len(offsets) - 1\n",
    "    total_edges = neighbors.size\n",
    "    degrees = offsets[1:] - offsets[:-1]\n",
    "    print(\"\\n==== [输入验证器] 初步分析图结构 ====\")\n",
    "    print(f\"总节点数: {total_nodes:,}\")\n",
    "    print(f\"总边数: {total_edges:,}\")\n",
    "\n",
    "    # 第1阶段：正常分组\n",
    "    initial_nodes = total_nodes\n",
    "    max_groups = total_edges // 4\n",
    "    neighbor_groups = np.full((max_groups, 4), -1, dtype=np.int32)\n",
    "    sources = np.full(max_groups, -1, dtype=np.int32)\n",
    "    used = np.zeros(total_nodes, dtype=np.uint8)\n",
    "    group_count = np.zeros(1, dtype=np.int32)\n",
    "    order = np.lexsort((-prank_scores, degrees)).astype(np.int32)\n",
    "    _grouping_core_with_real_edges(order, offsets, neighbors, used, neighbor_groups, sources, group_count)\n",
    "    valid_ng = neighbor_groups[:group_count[0]]\n",
    "    valid_src = sources[:group_count[0]]\n",
    "    group_storage = valid_ng.size\n",
    "    used_count = int(used.sum())\n",
    "    missing_count = total_nodes - used_count\n",
    "    first_storage = group_storage + missing_count\n",
    "    print(f\"[第一阶段] 有效组数量: {group_count[0]:,}\")\n",
    "    print(f\"[第一阶段] 组内存储: {group_storage:,}, 未覆盖节点: {missing_count:,}\")\n",
    "    print(f\"[第一阶段] 存储膨胀率: {first_storage/total_nodes:.2f}x\")\n",
    "\n",
    "    # 第2阶段：真实边冗余补组\n",
    "    new_ng, new_src = [], []\n",
    "    for node in np.where(used == 0)[0]:\n",
    "        current_storage = group_storage + (total_nodes - used_count)\n",
    "        if current_storage / total_nodes >= 1 + redundancy_rate:\n",
    "            break\n",
    "        nbrs = neighbors[offsets[node]:offsets[node+1]]\n",
    "        if len(nbrs) >= 4:\n",
    "            selected = nbrs[:4]\n",
    "            new_ng.append(selected)\n",
    "            new_src.append(node)\n",
    "            group_storage += 4\n",
    "            for nbr in selected:\n",
    "                if not used[nbr]:\n",
    "                    used[nbr] = 1\n",
    "                    used_count += 1\n",
    "    new_count = len(new_src)\n",
    "    if new_count:\n",
    "        new_ng = np.vstack(new_ng)\n",
    "        new_src = np.array(new_src, dtype=np.int32)\n",
    "    else:\n",
    "        new_ng = np.empty((0,4), dtype=np.int32)\n",
    "        new_src = np.empty((0,), dtype=np.int32)\n",
    "    print(f\"[第二阶段] 补充组数量: {new_count:,}\")\n",
    "\n",
    "    # 合并并统计\n",
    "    all_ng = np.vstack([valid_ng, new_ng])\n",
    "    all_src = np.concatenate([valid_src, new_src])\n",
    "    group_nodes = all_ng.flatten()\n",
    "    final_missing = total_nodes - used.sum()\n",
    "    final_storage = group_nodes.size + final_missing\n",
    "    print(\"\\n==== 最终存储统计 ====\")\n",
    "    print(f\"组内邻居节点数 (计重复): {group_nodes.size:,}\")\n",
    "    print(f\"组外未覆盖节点数: {final_missing:,}\")\n",
    "    print(f\"最终总存储节点数: {final_storage:,}\")\n",
    "    print(f\"总存储膨胀率: {final_storage/total_nodes:.2f}x\")\n",
    "\n",
    "    return all_src, all_ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dfeb97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== [输入验证器] 初步分析图结构 ====\n",
      "总节点数: 100,000,000\n",
      "总边数: 393,182,914\n",
      "[第一阶段] 有效组数量: 7,172,738\n",
      "[第一阶段] 组内存储: 28,690,952, 未覆盖节点: 71,337,928\n",
      "[第一阶段] 存储膨胀率: 1.00x\n",
      "[第二阶段] 补充组数量: 10,267,098\n",
      "\n",
      "==== 最终存储统计 ====\n",
      "组内邻居节点数 (计重复): 69,759,344\n",
      "组外未覆盖节点数: 70,240,659.0\n",
      "最终总存储节点数: 140,000,003.0\n",
      "总存储膨胀率: 1.40x\n"
     ]
    }
   ],
   "source": [
    "prank = torch.load('pr_large_all.pt').numpy()\n",
    "srcs, groups = full_optimized_grouping_with_real_edges(filtered, prank, redundancy_rate=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed07c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_groups_with_edges_allow_self_loop(all_sources, all_neighbor_groups, edges):\n",
    "    edge_set = set((int(s), int(d)) for s, d in edges)\n",
    "    total = len(all_sources)\n",
    "    valid = 0\n",
    "    for i in range(total):\n",
    "        src = int(all_sources[i])\n",
    "        nbrs = all_neighbor_groups[i]\n",
    "        ok = True\n",
    "        for d in nbrs:\n",
    "            if src == d:\n",
    "                continue\n",
    "            if (src, int(d)) not in edge_set:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok:\n",
    "            valid += 1\n",
    "    print(\"\\n==== 分组验证结果（允许自环） ====\")\n",
    "    print(f\"总组数: {total:,}\")\n",
    "    print(f\"验证通过的组数: {valid:,}\")\n",
    "    print(f\"验证失败的组数: {total-valid:,}\")\n",
    "    return valid, total-valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf54e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = np.load('/home/embed/Downloads/igb_large/processed/paper__cites__paper/edge_index.npy')\n",
    "edges = raw.T if raw.shape[0]==2 else raw[:, :2]\n",
    "validate_groups_with_edges_allow_self_loop(srcs, groups, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3d25eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[新节点序列] 总长度 140,000,003 = 29,031,448 (孤立) + 41,209,211 (剩余) + 69,759,344 (原始分组)\n"
     ]
    }
   ],
   "source": [
    "grouped_nodes = groups.flatten()\n",
    "total_nodes = len(stats2['offsets']) - 1\n",
    "used_iso  = np.zeros(total_nodes, dtype=bool)\n",
    "used_iso[iso_grouped] = True\n",
    "used_orig = np.zeros(total_nodes, dtype=bool)\n",
    "used_orig[grouped_nodes] = True\n",
    "remaining = np.where(~(used_iso | used_orig))[0].astype(np.uint32)\n",
    "\n",
    "new_node_sequence = np.concatenate([\n",
    "    iso_grouped,\n",
    "    remaining,\n",
    "    grouped_nodes\n",
    "])\n",
    "print(f\"[新节点序列] 总长度 {new_node_sequence.size:,} = \"\n",
    "    f\"{iso_grouped.size:,} (孤立) + {remaining.size:,} (剩余) + {grouped_nodes.size:,} (原始分组)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
